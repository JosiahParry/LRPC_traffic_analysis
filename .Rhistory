summarise(avg = mean(total),
count_hi = max(total),
count_low = min(total),
sd = sd(total),
se = sd(total)/sqrt(length(total)))
counters_13 %>%
group_by(counter, wday) %>%
summarise(avg = mean(total),
count_hi = max(total),
count_low = min(total),
sd = sd(total),
se = sd(total)/sqrt(length(total)))
counters_13 %>%
group_by(counter, date, wday) %>%
summarise(avg = mean(total),
count_hi = max(total),
count_low = min(total),
sd = sd(total),
se = sd(total)/sqrt(length(total)))
library(mclust)
library(mclust)
model <- Mclust(select(x, counter, date, wday *total))
model <- Mclust(select(x, total))
model <- Mclust(select(x, counter, total))
model
summary(model)
plot(model)
EM
library(nnet)
en
e
em
?em
model
summary(model)
model <- Mclust(select(x, counter, total * date))
model <- Mclust(select(x, counter, total*date))
faithful
t4_mod <- Mclust(select(tier_4, counter, date, total))
summary(t4_mod)
t4_mod$G
plot(t4_mod)
summary(t4_mod)
plot(t4_mod)
t4_mod <- Mclust(select(tier_4, counter, total))
summary(t4_mod)
plot(t4_mod)
t4_mod <- Mclust(select(tier_4, date, total))
summary(t4_mod)
plot(td_mod)
plot(t4_mod)
unique(tier_4$counter)
?Mclust
plot(date ~ total, data = tier_4)
plot(total ~ date, data = tier_4)
plot(total, date, data = tier_4)
plot(total ~ as.Date(date), data = tier_4)
plot(total ~ as.Date(date), data = tier_4, col = counter)
plot(total ~ as.Date(date), data = tier_4, col = wday)
50/365
.4/.6
.8 * .4
.8 * .24
.8 * .4
.6 * .3
.32 + .18
.32 / .5
.18/.5
50/365
h <- 50/365
d_h <- .8
d_h1 <- .3
d_h <- (d_h * h)/ (d_h * h) + (d_h1 * h)
d_h
d_h <- (d_h * h)/ (d_h * h) + (d_h1 * (1-h))
d_h
d_h <- (d_h * h)/ ((d_h * h) + (d_h1 * (1-h)))
d-H
d_h
prior <- 50/365
likelihood <- .8 * .3
likelihood * prior / .8
(likelihood * prior) / .8
likelihood <- .8 / (1-.3)
likelihood * prior / .8
likelihood <- .8 / .3
likelihood
likelihood * prior / .8
likelihood * prior / .3
likelihood * prior / (1-.8)
likelihood * prior / (1-.3)
likelihood * prior / [(.8 * prior) + (.3 * prior)]
likelihood * prior / [(.8 * prior) + (.3 * prior)]
likelihood * prior / ((.8 * prior) + (.3 * prior))
pH <- .50/365
pDH <- .8
pDH1 <- .3
pHD <- pDH*PH / pDH*pH + pDH1*(1 - pH)
pHD <- pDH*pH / pDH*pH + pDH1*(1 - pH)
phd
pHD
pH <- 50/365
pDH <- .8
pDH1 <- .3
pHD <- pDH*pH / pDH*pH + pDH1*(1 - pH)
pHD
.8*.14
.3(1-.14)
.3*(1-.14)
.3*(1-.14) + .112
.258 + .112
pH <- .14
pDH <- .8
pDH1 <- .3
pHD <- pDH*pH / pDH*pH + pDH1*(1 - pH)
pHD
pH <- .14
pDH <- .8
pDH1 <- .3
pHD <- pDH*pH / pDH*pH + pDH1*(1 - pH)
pHD
(.8 * .14) / (.8 * .14) + (.3 *(1-.14))
.8 * .14
(.8 * .14) / ((.8 * .14) + (.3 *(1-.14)))
.3(1-.14)
.3(1- * .14)
.3(1-  .14)
.3*(1-.14)
.258 + .112
.112/.37
pHD <- pDH*pH / pDH*pH + pDH1*(1 - pH)
pHD
(.8 * .14) / ((.8 * .14) + (.3 *(1-.14)))
.5 / 4
pd <- .2 * (.125) + .4*.125 + .5*.5 + .6*.125 + .8*.125
pd
pwh_d <- .8 *.125
psh_d <- .8 * .125
pwh_d <- .6 *.125
pd <- .2 * .125 + .4*.125 + .5*.5 + .6*.125 + .8*.125
wh <- .6
sh <- .8
pd_sh <- .8 * .125
pd_wd <- .6 *.125
wh <- .6
sh <- .8
pd <- .2 * .125 + .4*.125 + .5*.5 + .6*.125 + .8*.125
pd_wh <- .6 *.125
pwh_d <- pd_wh * wh / (pd)
pwh_d
pd
pwh_d <- pd_wh^3 * wh / (pd)
pw_d
pwh_d
pd <- .2^3 * .125 + .4^3*.125 + .5^3*.5 + .6^3*.125 + .8^3*.125
pwh_d <- pd_wh^3 * wh / (pd)
pwh_d
pd <- ((.2^3) * .125 )+ ((.4^3)*.125) + ((.5^3)*.5) +( (.6^3)*.125) + ((.8^3)*.125)
pwh_d <- pd_wh^3 * wh / (pd)
pwh_d
pwh_d <-( (pd_wh^3) * wh) / (pd)
pwh_d
pwh_d <- ((pd_wh^3) * wh) + ((pd_sh^3) * sh)/ (pd)
pwh_d
denominator <- numerator + (.4^3) * (.125) + (.5^3) * (.5) + (.2^3) * (.125)
numerator <- (.6)^3 * (.125) + (.8)^3*(.125)
denominator <- numerator + (.4^3) * (.125) + (.5^3) * (.5) + (.2^3) * (.125)
numerator / denominator
x <- (.6)^3 * (.125)
x/ denominator
y <- (.8)^3*(.125)
y / denominator
x + y / denominator
(x + y) / denominator
(x/ denominator) + (y / denominator)
devtools::install_github("StatsWithR/statsr")
x <- 4
sqrt(x)
x * sqrt(x)
1/.75
4*10.5
42*5
250/24
.2 * .2
.04 * 4
.16 / (.04 + 20)
sqrt(.16 / (.04 + 20))
.7 * .5
b <-1
x <- 3
b <-1
a <- 1
b <-1
x <- 3
n <- 24
a + x
b + n - x
a1 <-a + x
b2 <- b + n - x
a1 / (a1 + b2)
a1
a1 + b2
a1 / 26
a1 / 26 (-1)
a1 / 26 - 1
View(tier_4)
range(tier_4$date)
range(tier_4$month)
2/5
.4/.1
.1/4
gc()
lrpc <- readRDS("data/lrpc.RDS")
lrpc
fctr_grp_15 <- read_csv("data/15_factor_grp.csv")
library(tidyverse)
fctr_grp_15 <- read_csv("data/15_factor_grp.csv")
fctr_grp_15
fctr_grp_15
fctr_grp_15
fctr_grp_15 <- read.csv("data/15_factor_grp.csv")
fctr_grp_15
x <- inner_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER"))
fctr_grp_15 <- read_csv("data/15_factor_grp.csv")
fctr_grp_15
x <- left_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER"))
?read_csv
?col_types
fctr_grp_15 <- read_csv("data/15_factor_grp.csv",
col_types =
list(col_double(), col_character(), col_character(), col_character()))
x <- left_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER"))
x
View(x)
?left_join
x <- left_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER")) %>% distinct(counter)
x <- left_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER")) %>% distinct(COUNTER)
x
x <- left_join(fctr_grp_15, lrpc, by = c("COUNTER"= "COUNTER"))
View(x)
x <- left_join(lrpc, fctr_grp_15 by = c("COUNTER"= "COUNTER"))
x <- left_join(lrpc, fctr_grp_15, by = c("COUNTER"= "COUNTER"))
x <- inner_join(lrpc, fctr_grp_15, by = c("COUNTER"= "COUNTER"))
View(x)
lprc_fctrs_15 <- inner_join(lrpc, fctr_grp_15, by = c("COUNTER"= "COUNTER"))
View(lprc_fctrs_15)
lprc_fctrs_15$LOCATION
View(lrpc)
lrpc$TOWN_NAME
unique(lrpc$TOWN_NAME)
View(lrpc)
lrpc_roads <- read_csv("../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
lrpc_roads$`_TOWN_NAME`
unique(lrpc_roads$`_TOWN_NAME`)
read_csv("data/All_LRPCcounts.txt")
lrpcs_all <- read_csv("data/All_LRPCcounts.txt")
View(lrpc)
lrpc_all <- read_csv("data/All_LRPCcounts.txt")
rm(lrpcs_all)
View(lrpc_all)
col_types
sapply(lrpc_all, class)
y <- sapply(lrpc_all, class)
y
names(y) <- NULL
y
lrpc_roads <- read_csv("../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
all <- read_csv(file = "../Data/All_LRPCcounts.csv") %>%
select(-FID)
rm(list = ls())
---
title: "LRPC Traffic Counter Location Cleaning"
output: html_notebook
---
```{r echo = FALSE}
library(tidyverse)
```
The goal of this notebook is to clean, and standardize road names associated with traffic counter locations used by the Lakes Region Planning Commision (LRPC) of New Hampshire. The current data used by the **LRPC** is not in accordance with standard practice of the *NH DOT* and therefore difficult to associate with shapefiles provided by *NH GRANIT*, and other data provided by *NH DOT*.
The first step in this is to read in all of the counter locations used by the *LRPC*, then the attribute table of the *NH GRANIT* roads shapefile. The first aim is to limit the NH Roads (attribute table of NH GRANIT roads shapefile) to the same towns as listed in the *LRPC* counter attribute table.
```{r}
# Read in LRPC counter locations
all <- read_csv(file = "../Data/All_LRPCcounts.csv") %>%
select(-FID)
# Read in the NH GRANIT attribute table
nh_roads <- read_csv("../../GIS/geo_spat/NH_roads/NH_roads.csv")
```
In order to match the towns, the towns within the `all` (LRPC counters) need to be isolated, and `nh_roads` (NH GRANIT attribute table) should be filtered to only include those towns.
```{r}
# Isolate LRPC towns
lrpc_towns <- unique(all$Town)
# Print LRPC towns
lrpc_towns
```
Note there are 30 towns (excluding the blank `""`) that are covered in the LRPC data. Now the `nh_roads` must be filtered. There should only be 30 towns in the filtered data.
```{r}
# Filter
lrpc_roads <- nh_roads %>%
filter(TOWN_NAME %in% lrpc_towns)
# View filtered town names
unique(lrpc_roads$TOWN_NAME)
```
This method of matching proved to be immensely difficult. In order to most effectively match road to counter I loaded the *LRPC Counter* shapefile and the *NH Roads* shapefile into the open source geographic information system (GIS) platform **QGIS** (Quantum GIS). Initially I planned to perform an intersection as a method of geospatially joining the data, however, as with most geospatial data, the two vector files did not overlap perfectly, as not all data is going to be perfectly oriented in space. Thus required the employment of a nearest neighbor join using the *NNJoin* plugin which can be downloaded within *QGIS*. I then selectively deleted redundant fields and changed a few field names. The shapefile was exported to a **csv** and will be cleaned with **R**.
```{r}
# Load exported file from QGIS
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
```
Upon inspection of the data the field `COMBNUMS` contains some observations with counter names that are associated with another counter in the format `XXXXXXXX:XXXXXXXX,XXXXXXXX`. In order to have the proper data related to each counter, the observations with the above mentioned format must be split into 3 observations: one for each counter name in the above mentioned format.
```{r}
lrpc_roads <- lrpc_roads %>%
mutate(COMBNUMS = strsplit(COMBNUMS, "[, ]+")) %>%
unnest(COMBNUMS)
lrpc_roads$COMBNUMS <- lrpc_roads$COMBNUMS %>% stringr::str_replace_all(":", "")
```
Preview the newly formatted `lrpc_roads$COMBNUMS`:
```{r}
head(lrpc_roads$COMBNUMS, n = 20)
```
Now time to reformat the column titles:
```{r}
colnames(lrpc_roads) <- colnames(lrpc_roads) %>% stringr::str_replace("_", "")
```
Now the editing of this file is complete and can be written to a *csv*.
```{r}
head(lrpc_roads, n = 20)
#write_csv(lrpc_roads, "data/lrpc_counters.csv")
```
library(tidyverse)
all <- read_csv(file = "../Data/All_LRPCcounts.csv") %>%
select(-FID)
# Read in LRPC counter locations
all <- read_csv(file = "../Data/All_LRPCcounts.csv") %>%
select(-FID)
# Read in the NH GRANIT attribute table
nh_roads <- read_csv("../../GIS/geo_spat/NH_roads/NH_roads.csv")
# Isolate LRPC towns
lrpc_towns <- unique(all$Town)
# Print LRPC towns
lrpc_towns
lrpc_towns
lrpc_roads <- nh_roads %>%
filter(TOWN_NAME %in% lrpc_towns)
unique(lrpc_roads$TOWN_NAME)
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
lrpc_roads$Town
unique(lrpc_roads$Town)
lrpc_roads <- lrpc_roads %>%
mutate(COMBNUMS = strsplit(COMBNUMS, "[, ]+")) %>%
unnest(COMBNUMS)
lrpc_roads
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv")
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv") %>% select(-X, -Y)
?read_csv
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv") %>% select(-X, -Y)
lrpc_roads <- lrpc_roads %>%
mutate(COMBNUMS = strsplit(COMBNUMS, "[, ]+")) %>%
unnest(COMBNUMS)
lrpc_roads$COMBNUMS <- lrpc_roads$COMBNUMS %>% stringr::str_replace_all(":", "")
head(lrpc_roads$COMBNUMS, n = 20)
colnames(lrpc_roads) <- colnames(lrpc_roads) %>% stringr::str_replace("_", "")
head(lrpc_roads, n = 20)
write_csv(lrpc_roads, "data/lrpc_counters.csv")
write_csv(lrpc_roads, "../data/lrpc_counters.csv")
# Read in LRPC counter locations
all <- read_csv(file = "../Data/All_LRPCcounts.csv", progress = F) %>%
select(-FID)
# Read in the NH GRANIT attribute table
nh_roads <- read_csv("../../GIS/geo_spat/NH_roads/NH_roads.csv", progress = F)
head(lrpc_roads, n = 20)
#write_csv(lrpc_roads, "../data/lrpc_counters.csv")
lrpc_all <- read_csv("data/All_LRPCcounts.txt")
lrpc_all <- read_csv("data/All_LRPCcounts.csv")
unique(lrpc_all$Town)
lrpc_counters <- read_csv("data/lrpc_counters.csv")
unique(lrpc_counters$TOWNDOT)
sort(unique(lrpc_all$Town))
c <- (Alexandria, Alton, Andover, Ashland, Barnstead, Belmont, Bridgewater, Bristol, Center Harbor, Danbury, Effingham, Franklin, Freedom, Gilford, Gilmanton, Hebron, Hill, Holderness, Laconia, Meredith, Moultonborough, New Hampton, Northfield, Ossipee, Sanbornton, Sandwich, Tamworth, Tilton, Tuftonboro, Wolfeboro
lrpc <- read_csv("../data/lrpc_counters.csv")
sort(unique(na.omit(lrpc$Town)))
lrpc_towns <- sort(unique(na.omit(lrpc$Town)))
length(lrpc_towns)
library(tidyverse)
# Read in LRPC counter locations
all <- read_csv(file = "../Data/All_LRPCcounts.csv", progress = F) %>%
select(-FID)
# Read in the NH GRANIT attribute table
nh_roads <- read_csv("../../GIS/geo_spat/NH_roads/NH_roads.csv", progress = F)
# Isolate LRPC towns
lrpc_towns <- unique(all$Town)
# Print LRPC towns
lrpc_towns
# Filter
lrpc_roads <- nh_roads %>%
filter(TOWN_NAME %in% lrpc_towns)
# View filtered town names
unique(lrpc_roads$TOWN_NAME)
# Load exported file from QGIS
lrpc_roads <- read_csv("../../GIS/geo_spat/lrpc_roads/lrpc_roads.csv") %>% select(-X, -Y)
lrpc_roads <- lrpc_roads %>%
mutate(COMBNUMS = strsplit(COMBNUMS, "[, ]+")) %>%
unnest(COMBNUMS)
lrpc_roads$COMBNUMS <- lrpc_roads$COMBNUMS %>% stringr::str_replace_all(":", "")
lrpc_roads <- lrpc_roads %>%
mutate(COMBNUMS = strsplit(COMBNUMS, "[, ]+")) %>%
unnest(COMBNUMS)
lrpc_roads$COMBNUMS <- lrpc_roads$COMBNUMS %>% stringr::str_replace_all(":", "")
head(lrpc_roads$COMBNUMS, n = 20)
colnames(lrpc_roads) <- colnames(lrpc_roads) %>% stringr::str_replace("_", "")
head(lrpc_roads, n = 20)
#write_csv(lrpc_roads, "../data/lrpc_counters.csv")
fctr_grp_15 <- read_csv("../data/15_factor_grp.csv",
col_types =
list(col_double(), col_character(), col_character(), col_character()))
fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_towns <- sort(unique(na.omit(lrpc$Town)))
length(lrpc_towns)
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters
---
title: "R Notebook"
output: html_notebook
keep_md: TRUE
---
```{r, echo=F}
library(tidyverse)
```
# Question:
## Of the counters used to calculate seasonal averages, how many of them are in the lakes region?
```{r}
lrpc <- read_csv("../data/lrpc_counters.csv")
fctr_grp_15 <- read_csv("../data/15_factor_grp.csv",
col_types =
list(col_double(), col_character(), col_character(), col_character()))
head(lrpc)
head(fctr_grp_15)
```
The `lrpc` data frame contains all of the counters used by the **LRPC**, and the `fctr_grp_15` identifies all of the counters used by the **NH DOT** in the calculation of *seasonal adjustment factors*. The question to be assessed is: how many of these counters used by the **NH DOT** are within the **LRPC's** area? The hypothesis is that the Lakes Region is a very unique place with respect to recreation and tourism and therefore may be an anomoly in traffic analysis, and thus it's average traffic count may be dampened by other counters within it's same adjustment factor grouping.
To identify which factors are the **LRPC's** in the seasonal adjustment factor evaluation I will identify the counters from `fctr_grp_15` which are in the 30 towns of the Lakes Region.
The following code isolates all of the towns in the Lakes Region.
```{r}
lrpc_towns <- sort(unique(na.omit(lrpc$Town)))
length(lrpc_towns)
```
Next step is to isolate the counters that are used in the seasonal adjustment factor creation that are *within* the Lakes Region.
```{r}
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters
```
library(tidyverse)
lrpc <- read_csv("../data/lrpc_counters.csv")
fctr_grp_15 <- read_csv("../data/15_factor_grp.csv",
col_types =
list(col_double(), col_character(), col_character(), col_character()))
head(lrpc)
head(fctr_grp_15)
lrpc_towns <- sort(unique(na.omit(lrpc$Town)))
length(lrpc_towns)
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters
library(tidyverse)
lrpc <- read_csv("../data/lrpc_counters.csv")
fctr_grp_15 <- read_csv("../data/15_factor_grp.csv",
col_types =
list(col_double(), col_character(), col_character(), col_character()))
head(lrpc)
head(fctr_grp_15)
lrpc_towns <- sort(unique(na.omit(lrpc$Town)))
length(lrpc_towns)
lrpc_saf_counters <- fctr_grp_15 %>%
filter(TOWN %in% lrpc_towns)
lrpc_saf_counters
rm(list = ls())
list_dat_dirs <- function(dir, extension = "RAW", case_sensitive = FALSE) {
case_sensitive <- ifelse(case_sensitive == TRUE, FALSE, TRUE)
list.dirs(dir)[str_detect(list.dirs(dir),
fixed(extension, ignore_case = case_sensitive))]
}
list_dat_dirs("data/permanent_counts")
list_dat_dirs("data/permanent_counts")
list_dat_dirs("data/permanent_counts")
library(tidyr)
library(stringr)
list_dat_dirs("data/permanent_counts", extension = str_detect("_13_15"))
list_dat_dirs("data/permanent_counts", extension = str_detect("1315"))
list_dat_dirs("data/permanent_counts", extension = str_detect("13-15"))
list_dat_dirs("data/permanent_counts", extension = NA )
list_dat_dirs()
list_dat_dirs
list_dat_dirs("data/permanent_counts", extension = "RAW" )
list_dat_dirs("data/permanent_counts", extension = "RAW" )
library(readxl)
?read_excel
read_excel("data/permanent_counts/22039022_13-15/RAW")
list.files("data/permanent_counts/22039022_13-15/RAW")
read_excel("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx")
read_excel("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx")
system.file("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx", package = "readxl")
x <- system.file("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx", package = "readxl")
read_excel(x)
read_excel("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx")
read_excel("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx", skip = 10)
read_excel("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx", skip = 10)
read_lines("data/permanent_counts/22039022_13-15/RAW/MonthlyVolumeReport_1_2013.xlsx")
