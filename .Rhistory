I will review how I cleaned 2013 data using the above functions.
I manually converted raw counts from **TraxPro** to `.txt` comma separate files and organized them into sub-directories. The file structure looks like `ROOT > YEAR > WEEK > RAW`: where there are multiple years of data in the root directory, in my case, `2013 and 2014`. Each year has a number of weeks that data was collected, and within each week folder there is another folder containing the raw count data in a `.txt` file.
The root directory in this case is `"/Volumes/GIS/LRPC/2014"`, and the extension that contains the raw data is `"RAW"`. The first step is to obtain all of the path names containing raw data. I will assign these path names to a vector called `dat_dirs` (data directories). Note that this is the name of the main argument in the `read_counters()` function which requires a file path name.
```{r}
dat_dirs <- list_dat_dirs("/Volumes/GIS/LRPC/2014")
# Look at path names
dat_dirs
```
Next these path names need to be passed to `read_counters()` to be read and cleaned. But before they are read it is *imperative* that I know how many lines to skip before reading the file (this is best done through a manual inspection of the data).
Take a look at the data, how many lines should be skipped? The answer is three as seen (somewhat difficultly) below.
```{r}
tidy(read_lines("/Volumes/GIS/LRPC/2014/Week 1 - May 12/RAW/61141010.txt", n_max = 8))
```
Now that the path names are stored in `dat_dirs` and we know how many lines to skip we can read in the counts; but make sure to store them to a variable!
```{r}
counts_14 <- read_counters(dat_dirs, n_skip = 3)
```
Lets preview this data!
```{r}
head(counts_14, n = 10)
```
Now we have a data frame containing hourly counts for each counter.
However there are alternative ways of doing this. You can either call the `list_dat_dirs()` function in the place of the `dat_dirs` argument of `read_counters()`, **OR** you can pipe the `list_dat_dirs()` into the `read_counters()`function.
Below are both efficient ways of reading in raw counts:
```{r, eval = F}
counts <- read_counters(list_dat_dirs("/Volumes/GIS/LRPC/2014"), n_skip = 3)
counts <- list_dat_dirs("/Volumes/GIS/LRPC/2014") %>% read_counters(n_skip = 3)
```
---
title: "Cleaning Raw Traffic Counter"
subtitle: "Custom Functions"
author: "Josiah Parry"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(broom)
library(lubridate)
```
# `list_dat_dirs()`
During my first run through of cleaning the raw counter data I had to write down the file path of each raw count folder (or directory). In order to make this easier in the future I created the function `list_dat_dirs()`, or **list data directories**. The goal of this is to automate the collection of file path names.
The function is written as follows:
```{r}
list_dat_dirs <- function(dir, extension = "RAW", case_sensitive = FALSE) {
case_sensitive <- ifelse(case_sensitive == TRUE, FALSE, TRUE)
list.dirs(dir)[str_detect(list.dirs(dir),
fixed(extension, ignore_case = case_sensitive))]
}
```
### Nuances
This function will list all directories within a root directory with a specified ending extension. There are a few nuances (bugs, rather) of this function. The most important is that the target directories (those with the raw counts) should contain **only** raw count files; other nuances will be elaborated on in the following sections.
### Function Arguments
**`dir`**: *Short for directory*
This is the path name of root directory that contains sub-directories of counts. The directory path name should be in quotations.
- i.e. `"/Volumes/GIS/LRPC/2014"`
**`extension`**: *The name of the sub-directories of counts*
The default extension name is `"RAW"` due to the formatting of data on my hard drive. This can be changed to anything, and like the `dir` argument it should be in quotations; i.e. `"RAW"`. This allows the function to search for all directories with the name **RAW**
**`case_sensitve`**
A logical argument that determines if the function should look for directories with, or without respect to case sensitivity; the default if `FALSE`.
### Example
This example will demonstrate the output / uses of the function. This example will draw directly from the raw counter cleaning I have done.
```{r}
list_dat_dirs("/Volumes/GIS/LRPC/2014")
```
The output of this function is a vector of strings containing the path names of the directories that contain the raw output from **Traxpro**, and will be fed directly into the next function `read_counters()`.
******
# `read_counters()`
This function will read in raw counts and aggregate them together, while performing some mild cleaning and alterations. This function requires that all files that it reads be comma separated. This would be either in the form of **.csv** or any other text file delimted by commas. In the case of the **LRPC** data that is exported from **Traxpro** in **ASCII** format is a **.txt** extension * **comma separated** * file.
When reading in raw counts directionality is lost, only totals are given. If this becomes a problem, contact me, or submit an issue request so I can fix it.
The function is written as follows:
```{r}
read_counters <- function(dat_dirs, n_skip = 0) {
all_files <- list()
for (i in 1:length(dat_dirs)) {
for (j in 1:length(list.files(dat_dirs[i]))) {
all_files[length(list.files(dat_dirs[1:i])) - (length(list.files(dat_dirs[i])) - j)] <-
list(read_csv(file.path(dat_dirs[i],list.files(dat_dirs[i])[j]),
skip = n_skip, col_names = TRUE,
col_types = cols(
Date = col_date("%m / %d / %Y")
)) %>%
mutate(counter = list.files(dat_dirs[i])[j] %>%
str_replace_all(".txt", "")) %>%
select(Date, Time, counter, everything()))
}
}
counter_clean <- function(df, total = "total") {
df <- data.frame(df)
if (dim(df)[2] > 4) {
df[, total] <-  apply(df[, -c(1:3)], 1, sum)
} else {
df[, total] <- df[, 4]
}
df$date_time <- ymd_hms(paste(df$Date, df$Time))
df <- dplyr::select(df, counter, Date, Time, date_time, total)
`colnames<-`(df, tolower(colnames(df)))
}
do.call("rbind", lapply(all_files, counter_clean))
}
```
### Function Arguments
**dat_dirs**: *Short for data directories*
This argument asks for a path name (string) to the raw counter data. For example: `"..LRPC/2014/Week 2 - May 19/RAW"`.
**n_skip**:
The number of lines to skip before reading in the **csv**. The default is `n_skip = 0`, however it will most likely need to be adjusted on the output of the data. For example the raw data from 2013 has the following format:
```{r, echo=F}
tidy(read_lines("/Volumes/GIS/LRPC/2013/06032013/raw/62295051.txt", n_max = 6))
```
**NOTE**: If more than 1 file is being read at a time, it is imperative that each file has the same number of lines to skip.
*****
# Cleaning workflow
1. Convert raw data to comma separated text files (**csvs**)
2. Organize raw csvs into directories containing only raw counts
3. Get raw count directory paths
3a. Feed raw count directory paths to `read_counters()`
### Case study
I will review how I cleaned 2013 data using the above functions.
I manually converted raw counts from **TraxPro** to `.txt` comma separate files and organized them into sub-directories. The file structure looks like `ROOT > YEAR > WEEK > RAW`: where there are multiple years of data in the root directory, in my case, `2013 and 2014`. Each year has a number of weeks that data was collected, and within each week folder there is another folder containing the raw count data in a `.txt` file.
The root directory in this case is `"/Volumes/GIS/LRPC/2014"`, and the extension that contains the raw data is `"RAW"`. The first step is to obtain all of the path names containing raw data. I will assign these path names to a vector called `dat_dirs` (data directories). Note that this is the name of the main argument in the `read_counters()` function which requires a file path name.
```{r}
dat_dirs <- list_dat_dirs("/Volumes/GIS/LRPC/2014")
# Look at path names
dat_dirs
```
Next these path names need to be passed to `read_counters()` to be read and cleaned. But before they are read it is *imperative* that I know how many lines to skip before reading the file (this is best done through a manual inspection of the data).
Take a look at the data, how many lines should be skipped? The answer is three as seen (somewhat difficultly) below.
```{r}
tidy(read_lines("/Volumes/GIS/LRPC/2014/Week 1 - May 12/RAW/61141010.txt", n_max = 8))
```
Now that the path names are stored in `dat_dirs` and we know how many lines to skip we can read in the counts; but make sure to store them to a variable!
```{r}
counts_14 <- read_counters(dat_dirs, n_skip = 3)
```
Lets preview this data!
```{r}
head(counts_14, n = 10)
```
Now we have a data frame containing hourly counts for each counter.
However there are alternative ways of doing this. You can either call the `list_dat_dirs()` function in the place of the `dat_dirs` argument of `read_counters()`, **OR** you can pipe the `list_dat_dirs()` into the `read_counters()`function.
Below are both efficient ways of reading in raw counts:
```{r, eval = F}
counts <- read_counters(list_dat_dirs("/Volumes/GIS/LRPC/2014"), n_skip = 3)
counts <- list_dat_dirs("/Volumes/GIS/LRPC/2014") %>% read_counters(n_skip = 3)
```
---
title: "Cleaning Raw Traffic Counter"
subtitle: "Custom Functions"
author: "Josiah Parry"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(broom)
library(lubridate)
```
# `list_dat_dirs()`
During my first run through of cleaning the raw counter data I had to write down the file path of each raw count folder (or directory). In order to make this easier in the future I created the function `list_dat_dirs()`, or **list data directories**. The goal of this is to automate the collection of file path names.
The function is written as follows:
```{r}
list_dat_dirs <- function(dir, extension = "RAW", case_sensitive = FALSE) {
case_sensitive <- ifelse(case_sensitive == TRUE, FALSE, TRUE)
list.dirs(dir)[str_detect(list.dirs(dir),
fixed(extension, ignore_case = case_sensitive))]
}
```
### Nuances
This function will list all directories within a root directory with a specified ending extension. There are a few nuances (bugs, rather) of this function. The most important is that the target directories (those with the raw counts) should contain **only** raw count files; other nuances will be elaborated on in the following sections.
### Function Arguments
**`dir`**: *Short for directory*
This is the path name of root directory that contains sub-directories of counts. The directory path name should be in quotations.
- i.e. `"/Volumes/GIS/LRPC/2014"`
**`extension`**: *The name of the sub-directories of counts*
The default extension name is `"RAW"` due to the formatting of data on my hard drive. This can be changed to anything, and like the `dir` argument it should be in quotations; i.e. `"RAW"`. This allows the function to search for all directories with the name **RAW**
**`case_sensitve`**
A logical argument that determines if the function should look for directories with, or without respect to case sensitivity; the default if `FALSE`.
### Example
This example will demonstrate the output / uses of the function. This example will draw directly from the raw counter cleaning I have done.
```{r}
list_dat_dirs("/Volumes/GIS/LRPC/2014")
```
The output of this function is a vector of strings containing the path names of the directories that contain the raw output from **Traxpro**, and will be fed directly into the next function `read_counters()`.
******
# `read_counters()`
This function will read in raw counts and aggregate them together, while performing some mild cleaning and alterations. This function requires that all files that it reads be comma separated. This would be either in the form of **.csv** or any other text file delimted by commas. In the case of the **LRPC** data that is exported from **Traxpro** in **ASCII** format is a **.txt** extension * **comma separated** * file.
When reading in raw counts directionality is lost, only totals are given. If this becomes a problem, contact me, or submit an issue request so I can fix it.
The function is written as follows:
```{r}
read_counters <- function(dat_dirs, n_skip = 0) {
all_files <- list()
for (i in 1:length(dat_dirs)) {
for (j in 1:length(list.files(dat_dirs[i]))) {
all_files[length(list.files(dat_dirs[1:i])) - (length(list.files(dat_dirs[i])) - j)] <-
list(read_csv(file.path(dat_dirs[i],list.files(dat_dirs[i])[j]),
skip = n_skip, col_names = TRUE,
col_types = cols(
Date = col_date("%m / %d / %Y")
)) %>%
mutate(counter = list.files(dat_dirs[i])[j] %>%
str_replace_all(".txt", "")) %>%
select(Date, Time, counter, everything()))
}
}
counter_clean <- function(df, total = "total") {
df <- data.frame(df)
if (dim(df)[2] > 4) {
df[, total] <-  apply(df[, -c(1:3)], 1, sum)
} else {
df[, total] <- df[, 4]
}
df$date_time <- ymd_hms(paste(df$Date, df$Time))
df <- dplyr::select(df, counter, Date, Time, date_time, total)
`colnames<-`(df, tolower(colnames(df)))
}
do.call("rbind", lapply(all_files, counter_clean))
}
```
### Function Arguments
**dat_dirs**: *Short for data directories*
This argument asks for a path name (string) to the raw counter data. For example: `"..LRPC/2014/Week 2 - May 19/RAW"`.
**n_skip**:
The number of lines to skip before reading in the **csv**. The default is `n_skip = 0`, however it will most likely need to be adjusted on the output of the data. For example the raw data from 2013 has the following format:
```{r, echo=F}
tidy(read_lines("/Volumes/GIS/LRPC/2013/06032013/raw/62295051.txt", n_max = 6))
```
**NOTE**: If more than 1 file is being read at a time, it is imperative that each file has the same number of lines to skip.
*****
# Cleaning workflow
1. Convert raw data to comma separated text files (**csvs**)
2. Organize raw csvs into directories containing only raw counts
3. Get raw count directory paths
3a. Feed raw count directory paths to `read_counters()`
### Case study
I will review how I cleaned 2013 data using the above functions.
I manually converted raw counts from **TraxPro** to `.txt` comma separate files and organized them into sub-directories. The file structure looks like `ROOT > YEAR > WEEK > RAW`: where there are multiple years of data in the root directory, in my case, `2013 and 2014`. Each year has a number of weeks that data was collected, and within each week folder there is another folder containing the raw count data in a `.txt` file.
The root directory in this case is `"/Volumes/GIS/LRPC/2014"`, and the extension that contains the raw data is `"RAW"`. The first step is to obtain all of the path names containing raw data. I will assign these path names to a vector called `dat_dirs` (data directories). Note that this is the name of the main argument in the `read_counters()` function which requires a file path name.
```{r}
dat_dirs <- list_dat_dirs("/Volumes/GIS/LRPC/2014")
# Look at path names
dat_dirs
```
Next these path names need to be passed to `read_counters()` to be read and cleaned. But before they are read it is *imperative* that I know how many lines to skip before reading the file (this is best done through a manual inspection of the data).
Take a look at the data, how many lines should be skipped? The answer is three as seen (somewhat difficultly) below.
```{r}
tidy(read_lines("/Volumes/GIS/LRPC/2014/Week 1 - May 12/RAW/61141010.txt", n_max = 8))
```
Now that the path names are stored in `dat_dirs` and we know how many lines to skip we can read in the counts; but make sure to store them to a variable!
```{r}
counts_14 <- read_counters(dat_dirs, n_skip = 3)
```
Lets preview this data!
```{r}
head(counts_14, n = 10)
```
Now we have a data frame containing hourly counts for each counter.
However there are alternative ways of doing this. You can either call the `list_dat_dirs()` function in the place of the `dat_dirs` argument of `read_counters()`, **OR** you can pipe the `list_dat_dirs()` into the `read_counters()`function.
Below are both efficient ways of reading in raw counts:
```{r, eval = F}
counts <- read_counters(list_dat_dirs("/Volumes/GIS/LRPC/2014"), n_skip = 3)
counts <- list_dat_dirs("/Volumes/GIS/LRPC/2014") %>% read_counters(n_skip = 3)
```
library(servr)
rmdv2()
x <- rmarkdown::render("write_ups/clean_join_counts.Rmd", run_pandoc = FALSE, clean = FALSE)
getwd()
lrpc <- readRDS("data/lrpc.RDS")
lrpc <- readRDS("/data/lrpc.RDS")
lrpc <- readRDS("../data/lrpc.RDS")
source("R/read_counters.R")
source("../R/read_counters.R")
source("../R/list_dat_dirs.R")
counts_13 <- list_dat_dirs("/Volumes/GIS/LRPC/2013") %>% read_counters(n_skip = 2)
counts_13 <- list_dat_dirs("/Volumes/GIS/LRPC/2013") %>% read_counters(n_skip = 2)
counts_14 <- list_dat_dirs("/Volumes/GIS/LRPC/2014") %>% read_counters(n_skip = 3)
full_counts_13 <- left_join(counts_13, lrpc, by = c("counter" = "COMBNUMS")) %>%
distinct(counter, date_time, total, .keep_all = T)
full_counts_14 <- left_join(counts_14, lrpc, by = c("counter" = "COMBNUMS")) %>%
distinct(counter, date_time, total, .keep_all = T)
lrpc <- readRDS("../data/lrpc.RDS")
source("../R/read_counters.R")
source("../R/list_dat_dirs.R")
counts_13 <- list_dat_dirs("/Volumes/GIS/LRPC/2013") %>% read_counters(n_skip = 2)
counts_14 <- list_dat_dirs("/Volumes/GIS/LRPC/2014") %>% read_counters(n_skip = 3)
full_counts_13 <- left_join(counts_13, lrpc, by = c("counter" = "COMBNUMS")) %>%
distinct(counter, date_time, total, .keep_all = T)
full_counts_14 <- left_join(counts_14, lrpc, by = c("counter" = "COMBNUMS")) %>%
distinct(counter, date_time, total, .keep_all = T)
#write_csv(full_counts_13, "data/clean_counts/counters_13.csv")
#write_csv(full_counts_14, "data/clean_counts/counters_14.csv")
x <- rmarkdown::render("write_ups/clean_join_counts.Rmd", run_pandoc = FALSE, clean = FALSE)
knit_meta <- attr(x, "knit_meta")
?rmarkdown::render
rmarkdown::render(   input = 'file.knit.md'    , knit_meta = knit_meta )
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd'    , knit_meta = knit_meta )
rmarkdown::render(   input = 'write_ups/clean_join_counts.md'    , knit_meta = knit_meta )
x <- rmarkdown::render("write_ups/clean_join_counts.Rmd", run_pandoc = FALSE, clean = FALSE)
knit_meta <- attr(x, "knit_meta")
rmarkdown::render(   input = 'write_ups/clean_join_counts.md'    , knit_meta = knit_meta )
x <- rmarkdown::render("write_ups/clean_join_counts.Rmd", run_pandoc = FALSE, clean = FALSE)
knit_meta <- attr(x, "knit_meta")
rmarkdown::render(   input = 'write_ups/clean_join_counts.md'    , knit_meta = knit_meta )
rmarkdown::render(   input = 'write_ups/clean_join_counts.md'    , knit_meta = knit_meta,
output_file = 'file.md'
, output_format=   md_document( ) )
library(rmarkdown)
rmarkdown::render(   input = 'write_ups/clean_join_counts.md'    , knit_meta = knit_meta,
output_file = 'file.md'
, output_format=   md_document( ) )
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd'    , knit_meta = knit_meta,
output_file = 'file.md'
, output_format=   md_document( ) )
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd'    , knit_meta = knit_meta)
?render
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd', knit_meta = knit_meta,
output_format = markdown)
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd', knit_meta = knit_meta,
output_format = md_document)
rmarkdown::render(   input = 'write_ups/clean_join_counts.Rmd', knit_meta = knit_meta,
output_format = md_document())
rm(list = ls())
gc()
library(tidyverse)
library(lubridate)
library(stringr)
#devtools::install_github("guiastrennec/ggplus")
library(ggplus)
DOT_counts <- readRDS("data/DOT_counts.RDS")
DOT_counts <- DOT_counts %>%
mutate(year = year(date),
day = wday(date, label = TRUE),
month = month(date, label = TRUE))
DOT_daily_avgs <- DOT_counts %>%
group_by(counter, year , month, day, date) %>%
summarise(daily_total = sum(count)) %>%
group_by(counter, year, month, day) %>%
summarise(avg_day = mean(daily_total, na.rm = TRUE)) %>%
mutate(ymd = ymd(sprintf('%04d%02d%02d', year, month, day)))
DOT_wday_avg <- DOT_counts %>%
filter(day %in% c("Mon", "Tues", "Wed", "Thurs", "Fri")) %>%
group_by(counter, year, month, day, date) %>%
summarise(daily_total = sum(count)) %>%
group_by(counter, year, month) %>%
summarise(avg_wday = mean(daily_total, na.rm = TRUE))
# Now I must split each data frame based on the group and year
# Then will calculate the averages for the group
# Read in factors
fctr_13_14 <- readRDS("data/factors_13_14.rds")
fctr_15 <- readRDS("data/factors_15.rds")
DOT_13_daily <- filter(DOT_daily_avgs, year == "2013") %>%
inner_join(fctr_13_14, by = "counter")
DOT_14_daily <- filter(DOT_daily_avgs, year == "2014") %>%
inner_join(fctr_13_14, by = "counter")
DOT_15_daily <- filter(DOT_daily_avgs, year == "2015") %>%
inner_join(fctr_15, by = c("counter" = "COUNTER"))
DOT_13_wday <- filter(DOT_wday_avg, year == "2013") %>%
inner_join(fctr_13_14, by = "counter")
DOT_14_wday <- filter(DOT_wday_avg, year == "2014") %>%
inner_join(fctr_13_14, by = "counter")
DOT_15_wday <- filter(DOT_wday_avg, year == "2015") %>%
inner_join(fctr_15, by = c("counter" = "COUNTER"))
## Calculate Sunday Factors
DOT_13_sun <- DOT_13_daily %>% filter(day == "Sun") %>%
group_by(GRP, month) %>%
summarise(avg_sun = mean(avg_day)) %>%
mutate(year_sun_avg = mean(avg_sun),
sun_fctr = avg_sun / year_sun_avg)%>%
arrange(GRP)
DOT_14_sun <- DOT_14_daily %>% filter(day == "Sun") %>%
group_by(GRP, month) %>%
summarise(avg_sun = mean(avg_day)) %>%
mutate(year_sun_avg = mean(avg_sun),
sun_fctr = avg_sun / year_sun_avg)%>%
arrange(GRP)
DOT_15_sun <- DOT_15_daily %>% filter(day == "Sun") %>%
group_by(GROUP, month) %>%
summarise(avg_sun = mean(avg_day)) %>%
mutate(year_sun_avg = mean(avg_sun),
sun_fctr = avg_sun / year_sun_avg)%>%
arrange(GROUP)
## Calculate Saturday Factors
DOT_13_sat <- DOT_13_daily %>% filter(day == "Sat") %>%
group_by(GRP, month) %>%
summarise(avg_sat = mean(avg_day)) %>%
mutate(year_sat_avg = mean(avg_sat),
sat_fctr = avg_sat / year_sat_avg)%>%
arrange(GRP)
DOT_14_sat <- DOT_14_daily %>% filter(day == "Sat") %>%
group_by(GRP, month) %>%
summarise(avg_sat = mean(avg_day)) %>%
mutate(year_sat_avg = mean(avg_sat),
sat_fctr = avg_sat / year_sat_avg)%>%
arrange(GRP)
DOT_15_sat <- DOT_15_daily %>% filter(day == "Sat") %>%
group_by(GROUP, month) %>%
summarise(avg_sat = mean(avg_day)) %>%
mutate(year_sat_avg = mean(avg_sat),
sat_fctr = avg_sat / year_sat_avg)%>%
arrange(GROUP)
## Calculate weekday factors
DOT_13_wday_fctr <- DOT_13_wday %>%
group_by(GRP, month) %>%
summarise(avg_wday = mean(avg_wday)) %>%
mutate(year_wday_avg = mean(avg_wday),
wday_fctr = avg_wday / year_wday_avg)
DOT_14_wday_fctr <- DOT_14_wday %>%
group_by(GRP, month) %>%
summarise(avg_wday = mean(avg_wday)) %>%
mutate(year_wday_avg = mean(avg_wday),
wday_fctr = avg_wday / year_wday_avg)
DOT_15_wday_fctr <- DOT_15_wday %>%
group_by(GROUP, month) %>%
summarise(avg_wday = mean(avg_wday)) %>%
mutate(year_wday_avg = mean(avg_wday),
wday_fctr = avg_wday / year_wday_avg)
## Calculate daily factors
DOT_13_day_fctr <- DOT_13_daily %>%
group_by(GRP, month) %>%
summarise(avg_day = mean(avg_day)) %>%
mutate(year_day_avg = mean(avg_day),
day_fctr = avg_day / year_day_avg)
DOT_14_day_fctr <- DOT_14_daily %>%
group_by(GRP, month) %>%
summarise(avg_day = mean(avg_day)) %>%
mutate(year_day_avg = mean(avg_day),
day_fctr = avg_day / year_day_avg)
DOT_15_day_fctr <- DOT_15_daily %>%
group_by(GROUP, month) %>%
summarise(avg_day = mean(avg_day)) %>%
mutate(year_day_avg = mean(avg_day),
day_fctr = avg_day / year_day_avg)
# Join the factors
DOT_13_fctr <- bind_cols(DOT_13_day_fctr, DOT_13_sat, DOT_13_sun, DOT_13_wday_fctr)[!duplicated(names(
bind_cols(DOT_13_day_fctr, DOT_13_sat, DOT_13_sun, DOT_13_wday_fctr)))]
DOT_14_fctr <- bind_cols(DOT_14_day_fctr, DOT_14_sat, DOT_14_sun, DOT_14_wday_fctr)[!duplicated(names(
bind_cols(DOT_14_day_fctr, DOT_14_sat, DOT_14_sun, DOT_14_wday_fctr)))]
DOT_15_fctr <- bind_cols(DOT_15_day_fctr, DOT_15_sat, DOT_15_sun, DOT_15_wday_fctr)[!duplicated(names(
bind_cols(DOT_15_day_fctr, DOT_15_sat, DOT_15_sun, DOT_15_wday_fctr)))]
## Cleaning the working environment
rm(list = ls()[!ls() %in% c("DOT_13_fctr","DOT_14_fctr", "DOT_15_fctr")])
gc()
#---
#PLOTS
#gg <- ggplot(DOT_daily_avgs,
#       aes(x = ymd, y = avg_day, color = counter )) +
#  geom_line() +
#  geom_smooth() +
#  geom_point()
#gg_faceted <- facet_multiple(gg, facets = c("counter", "year"), ncol = 3, nrow = 3, scales = "free")
View(DOT_15_fctr)
rm(list = ls())
install.packages("pROC")
install.packages("pROC")
library(pROC)
data(segmentationData)
library(dplyr)
library(caret)
library(dplyr)
library(kernlab)
library(pROC)
data("segmentationData")
head(segmentationData)
library(tidyverse)
counters_14 <- read_csv("data/clean_counts/counters_14.csv")
coutners_14
counters_14
library(caret)
?trainControl
control <- trainControl(method = "repeatedcv",
repeats = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE)
?createDataPartition
?train
x <- readRDS("data/DOT_counts.rds")
x
x<- readRDS("data/factors_13_14.rds")
c
x
load("R/read_NHDOT.R")
source("R/read_NHDOT.R")
DOT_counts <- readRDS("data/DOT_counts.RDS")
library(lubridate)
library(stringr)
DOT_counts <- readRDS("data/DOT_counts.RDS") %>%
mutate(year = year(date),
day = wday(date, label = TRUE),
month = month(date, label = TRUE))
glimpse(segmentationData[, 1:3])
DOT_counts_13_14 <- DOT_counts %>%
filter(year %in% c(2013, 2014))
4, readRDS("data/factors_13_14.rds"),
by = "counter")
DOT_counts_13_14 <- inner_join(DOT_counts_13_14, readRDS("data/factors_13_14.rds"),
by = "counter")
DOT_counts_13_14 <- DOT_counts %>%
filter(year %in% c(2013, 2014))
DOT_counts_13_14 <- inner_join(DOT_counts_13_14, readRDS("data/factors_13_14.rds"),
by = "counter")
segmentationData$Case
trainIndex <- createDataPartition(DOT_counts_13_14, p = 0.5, list = FALSE)
?createDataPartition
?createTimeSlices
createTimeSlices(1:9, 5, 1, fixedWindow = TRUE)
install.packages("dtwcluster")
install.packages("dtwclust")
library(dtwclust)
?dtwclust
dtwclust(DOT_counts_13_14, type = "fuzzy",  k = 3)
dtwclust(DOT_counts_13_14$count, type = "fuzzy",  k = 3)
